{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the AWS_PROFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env AWS_PROFILE=dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading and preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dolly_dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "\n",
    "# To train for question answering/information extraction, you can replace the assertion in next line to example[\"category\"] == \"closed_qa\"/\"information_extraction\".\n",
    "summarization_dataset = dolly_dataset.filter(lambda example: example[\"category\"] == \"information_extraction\")\n",
    "summarization_dataset = summarization_dataset.remove_columns(\"category\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = summarization_dataset.train_test_split(test_size=0.1)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This template is tailored for the task of extracting specific information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "              \"Extract the requested information from the context and provide a concise response.\\n\\n\"\n",
    "              \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{context}\\n\\n\",\n",
    "    \"completion\": \"{response}\"\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the training data and also the tempalte to default s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "default_bucket_prefix = sagemaker.Session().default_bucket_prefix\n",
    "default_bucket_prefix_path = \"\"\n",
    "\n",
    "# If a default bucket prefix is specified, append it to the s3 path\n",
    "if default_bucket_prefix:\n",
    "    default_bucket_prefix_path = f\"/{default_bucket_prefix}\"\n",
    "\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}{default_bucket_prefix_path}/dolly_information_exatraction_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Role for Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Initialize the IAM client\n",
    "iam_client = boto3.client('iam')\n",
    "\n",
    "# Define the role name\n",
    "role_name = 'SageMakerExecutionRole'\n",
    "\n",
    "# Trust policy for SageMaker\n",
    "trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"Service\": \"sagemaker.amazonaws.com\"\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Custom S3 policy (optional, for specific bucket access)\n",
    "s3_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\n",
    "                \"s3:GetObject\",\n",
    "                \"s3:PutObject\",\n",
    "                \"s3:ListBucket\"\n",
    "            ],\n",
    "            \"Resource\": [\n",
    "                \"*\",\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Create the IAM role\n",
    "    response = iam_client.create_role(\n",
    "        RoleName=role_name,\n",
    "        AssumeRolePolicyDocument=json.dumps(trust_policy),\n",
    "        Description='Role for SageMaker to create training jobs'\n",
    "    )\n",
    "    role_arn = response['Role']['Arn']\n",
    "    print(f\"Created role with ARN: {role_arn}\")\n",
    "\n",
    "    # Attach AmazonSageMakerFullAccess managed policy\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn='arn:aws:iam::aws:policy/AmazonSageMakerFullAccess'\n",
    "    )\n",
    "    print(\"Attached AmazonSageMakerFullAccess policy\")\n",
    "\n",
    "    # (Optional) Create and attach a custom S3 policy\n",
    "    s3_policy_name = 'SageMakerS3AccessPolicy'\n",
    "    iam_client.create_policy(\n",
    "        PolicyName=s3_policy_name,\n",
    "        PolicyDocument=json.dumps(s3_policy)\n",
    "    )\n",
    "    iam_client.attach_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyArn=f'arn:aws:iam::{boto3.client(\"sts\").get_caller_identity()[\"Account\"]}:policy/{s3_policy_name}'\n",
    "    )\n",
    "    print(\"Attached custom S3 policy\")\n",
    "\n",
    "    # Wait briefly to ensure the role is propagated\n",
    "    time.sleep(10)\n",
    "\n",
    "except iam_client.exceptions.EntityAlreadyExistsException:\n",
    "    print(f\"Role {role_name} already exists. Retrieving ARN...\")\n",
    "    role_arn = iam_client.get_role(RoleName=role_name)['Role']['Arn']\n",
    "except Exception as e:\n",
    "    print(f\"Error creating role: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "model_id, model_version = \"meta-textgeneration-llama-3-8b\", \"2.*\"\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    environment={\"accept_eula\": \"true\"},  # Please change {\"accept_eula\": \"true\"}\n",
    "    disable_output_compression=True,\n",
    "    instance_type=\"ml.g5.12xlarge\",  # For Llama-3-70b, add instance_type = \"ml.g5.48xlarge\",\n",
    "    role=role_arn,\n",
    "    instance_count=1,\n",
    ")\n",
    "# By default, instruction tuning is set to false. Thus, to use instruction tuning dataset you use\n",
    "estimator.set_hyperparameters(instruction_tuned=\"True\", epoch=\"5\", max_input_length=\"1024\")\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = train_and_test_dataset[\"test\"]\n",
    "\n",
    "(\n",
    "    inputs,\n",
    "    ground_truth_responses,\n",
    "    responses_before_finetuning,\n",
    "    responses_after_finetuning,\n",
    ") = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": template[\"prompt\"].format(\n",
    "            instruction=datapoint[\"instruction\"], context=datapoint[\"context\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    print(\"_\"*20)\n",
    "\n",
    "    print(payload)\n",
    "    # inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    # Please change the following line to \"accept_eula=true\"\n",
    "    # pretrained_response = pretrained_predictor.predict(\n",
    "    #     payload, custom_attributes=\"accept_eula=false\"\n",
    "    # )\n",
    "    # responses_before_finetuning.append(pretrained_response.get(\"generated_text\"))\n",
    "    # Fine Tuned Llama 3 models doesn't required to set \"accept_eula=true\"\n",
    "    finetuned_response = finetuned_predictor.predict(payload)\n",
    "    print(finetuned_response.get(\"generated_text\"))\n",
    "    print(\"_\"*20)\n",
    "\n",
    "    responses_after_finetuning.append(finetuned_response.get(\"generated_text\"))\n",
    "\n",
    "\n",
    "for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "    predict_and_print(datapoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
